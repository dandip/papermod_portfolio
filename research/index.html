<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Research | Dan DiPietro</title>
<meta name=keywords content><meta name=description content="I enjoy lots of flavors of research. My favorite problems combine novel mathematical
thinking with challenging computational implementations.
I will do my best to provide high-level summaries of my work here&ndash;they probably won&rsquo;t
make sense without domain knowledge. I think there&rsquo;s no substitute for reading
the abstract or the paper itself (most of these are freely available).
Machine Learning (Computational Physics)
DiPietro, D. M., & Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian
Dynamical Systems. arXiv:2209.01521. Paper.

"><meta name=author content><link rel=canonical href=https://dandipietro.com/research/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://dandipietro.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dandipietro.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dandipietro.com/favicon-32x32.png><link rel=apple-touch-icon href=https://dandipietro.com/apple-touch-icon.png><link rel=mask-icon href=https://dandipietro.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dandipietro.com/research/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Research"><meta property="og:description" content="I enjoy lots of flavors of research. My favorite problems combine novel mathematical
thinking with challenging computational implementations.
I will do my best to provide high-level summaries of my work here&ndash;they probably won&rsquo;t
make sense without domain knowledge. I think there&rsquo;s no substitute for reading
the abstract or the paper itself (most of these are freely available).
Machine Learning (Computational Physics)
DiPietro, D. M., & Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian
Dynamical Systems. arXiv:2209.01521. Paper.

"><meta property="og:type" content="article"><meta property="og:url" content="https://dandipietro.com/research/"><meta property="article:section" content><meta name=twitter:card content="summary"><meta name=twitter:title content="Research"><meta name=twitter:description content="I enjoy lots of flavors of research. My favorite problems combine novel mathematical
thinking with challenging computational implementations.
I will do my best to provide high-level summaries of my work here&ndash;they probably won&rsquo;t
make sense without domain knowledge. I think there&rsquo;s no substitute for reading
the abstract or the paper itself (most of these are freely available).
Machine Learning (Computational Physics)
DiPietro, D. M., & Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian
Dynamical Systems. arXiv:2209.01521. Paper.

"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Research","item":"https://dandipietro.com/research/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Research","name":"Research","description":"I enjoy lots of flavors of research. My favorite problems combine novel mathematical thinking with challenging computational implementations.\nI will do my best to provide high-level summaries of my work here\u0026ndash;they probably won\u0026rsquo;t make sense without domain knowledge. I think there\u0026rsquo;s no substitute for reading the abstract or the paper itself (most of these are freely available).\nMachine Learning (Computational Physics) DiPietro, D. M., \u0026amp; Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian Dynamical Systems. arXiv:2209.01521. Paper. ","keywords":[],"articleBody":"I enjoy lots of flavors of research. My favorite problems combine novel mathematical thinking with challenging computational implementations.\nI will do my best to provide high-level summaries of my work here–they probably won’t make sense without domain knowledge. I think there’s no substitute for reading the abstract or the paper itself (most of these are freely available).\nMachine Learning (Computational Physics) DiPietro, D. M., \u0026 Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian Dynamical Systems. arXiv:2209.01521. Paper. Fundamental Question: How can we learn symbolic governing equations from observational data of physical dynamical systems? We present a novel machine learning framework for answering this question. It works in the following way: We use some specially constructed symplectic neural networks to deduce algebraic properties of the governing equations, such as additive separability. Next, an LSTM-RNN autoregressively generates a sequence of operators/system variables that corresponds to the pre-order traversal of an expression tree for some symbolic equation. Specifically, the LSTM-RNN is structured to generate separable physics equations called Hamiltonians. We transpile the expression trees into Pytorch code so that we can optimize their constants with auto-differentiation. We use the expression tree to time-evolve the physical system by symplectically integrating it. The error that we get in doing so is our loss function. Due to the transpiling step, we lose end-to-end differentiability. So, we train the LSTM-RNN with a reinforcement learning approach called risk-seeking policy gradients. We go back to step 2 and repeat until we’re satisfied with the generated equation. Code: https://github.com/dandip/SISR My Contribution: This was my senior thesis–I’m responsible for the implementation, nearly all of the ideation, and the paper. My advisor suggested step 1, which greatly improved performance. The variable-length batching code of the LSTM-RNN, as well as the transpiler, presented fun technical challenges. DiPietro, D. M., Xiong, S., \u0026 Zhu, B. (2020). Sparse Symplectically Integrated Neural Networks. Advances in Neural Information Processing Systems. Paper. Fundamental Question: How can we learn symbolic governing equations from observational data of physical dynamical systems? We present a novel machine learning framework for answering this question. It works in the following way: Pick some finite function space, i.e. 6th order polynomials, perhaps with some trigonometric terms. Implement a neural network that encodes this function space, with the coefficients of each term being trainable parameters. To obtain loss, symplectically integrate the neural network (making this a form of neural differential equation) and take the L1-error between its predicted future state of the physical system and the physical systems actual future state. Train the network until it converges (you have your equation) or fails to (pick a different function space). Code: https://github.com/dandip/ssinn My Contribution: My co-authors helped with the numerical integration code. I’m otherwise responsible for the ideation, implemenation, and paper. Deng, Y., Zhang, Y., He, X., Yang, S., Tong, Y., Zhang, M., DiPietro, D. M., \u0026 Zhu, B. (2020). Soft Multicopter Control using Neural Dynamics Identification. Conference on Robot Learning. Paper. Fundamental Question: How do we effectively control a soft-bodied drone? These drones can deform to get through small spaces and can crash without sustaining as much damage as a stiff drone. My Contribution: I wrote a soft-bodied physics engine used to generate training data for this paper. My implementation used the finite element method and a neo-Hookean model of elasticity. It was written in C++ using the Eigen library and can run in real-time. Machine Learning (Natural Language Processing) DiPietro, D. M. (2022). Quantitative Stopword Generation for Sentiment Analysis via Recursive and Iterative Deletion. arXiv:2209.01519. Paper.\nFundamental Question: How do we rigorously generate a set of stopwords? My technique: Take a pre-trained transformer (DistilBERT). Perform an iterative feature deletion on every word in its vocabulary. Rank the words in order of how much their deletion degrades performance (measured as AUC) on the test set. When training new models, my best stopword set reduced corpus size by 63.7% while only reducing accuracy by 2.8%. A smaller stopword set reduced corpus size by 16% without affecting performance. DiPietro, D. M., Hazari, V. D., \u0026 Vosoughi, S. (2022). Robin: A Novel Online Suicidal Text Corpus of Substantial Breadth and Scale. arXiv:2209.05707. Paper.\nFundamental Question: How do we create models that accurate classify suicidal text? We scrape 1.1m suicidal social media posts and use them to fine-tune a BERT model, achieving SOTA on this task. Code: https://github.com/dandip/DH_Kappa My Contribution: I wrote all of the code–otherwise even contribution amongst co-authors. Statistics Theory DiPietro, D. M., \u0026 Hazari, V. D. (2022). DiPietro-Hazari Kappa: A Novel Metric for Assessing Labeling Quality via Annotation. arXiv:2209.08243. Paper.\nFundamental Question: Suppose a dataset is labeled via some heuristic. The same dataset is then annotated by humans. How do we measure the “goodness” of our heuristic labels in the context of our human annotation labels? Our technique: We present a novel statistical/probabilistic approach inspired by Fleiss’s Kappa. At a high level, we measure the annotator agreement different that was attained above random chance. We also offer a performant matrix implementation. My Contribution: Ideation on this was 50/50 with my co-author. I formalized our math, derived the matrix formulation, and did the code implementation. Quantitative Finance Fleiss, A., Cui, H., Stoikov, S., \u0026 DiPietro, D. M. (2020). Constructing Equity Portfolios from SEC 13F Data Using Feature Extraction and Machine Learning. Journal of Financial Data Science, 2(1), 45-60. Paper. not open access :(\nFundamental Question: How can we use SEC 13F data to quantitatively inform long-term investments? We extract features and use them to train gradient-boosted trees. This strategy earned 19.8% annualized in historical backtesting versus 9.5% for the S\u0026P 500 over the same time period. DiPietro, D. M. (2019). Alpha Cloning: Using Quantitative Techniques and SEC 13F Data for Equity Portfolio Optimization and Generation. Journal of Financial Data Science, 1(4), 159-171. Paper. not open access :(\nFundamental Question: How can we use SEC 13F data to quantitatively inform long-term investments? I employ some naive heuristics that are able to beat the S\u0026P 500 over a five-year backtested period (95% vs 72%). ","wordCount":"995","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://dandipietro.com/research/"},"publisher":{"@type":"Organization","name":"Dan DiPietro","logo":{"@type":"ImageObject","url":"https://dandipietro.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dandipietro.com/ accesskey=h title="Dan DiPietro (Alt + H)">Dan DiPietro</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://dandipietro.com/ title=About><span>About</span></a></li><li><a href=https://dandipietro.com/research title=Research><span class=active>Research</span></a></li><li><a href=https://dandipietro.com/software title="Software Projects"><span>Software Projects</span></a></li><li><a href=https://dandipietro.com/CVDiPietro_curr.pdf title=CV><span>CV</span></a></li><li><a href=https://isomorphic.group title=Blog><span>Blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Research</h1></header><div class=post-content><p>I enjoy lots of flavors of research. My favorite problems combine novel mathematical
thinking with challenging computational implementations.</p><p>I will do my best to provide high-level summaries of my work here&ndash;they probably won&rsquo;t
make sense without domain knowledge. I think there&rsquo;s no substitute for reading
the abstract or the paper itself (most of these are freely available).</p><h2 id=machine-learning-computational-physics>Machine Learning (Computational Physics)<a hidden class=anchor aria-hidden=true href=#machine-learning-computational-physics>#</a></h2><p><strong>DiPietro, D. M.</strong>, & Zhu, B. (2022). Symplectically Integrated Symbolic Regression of Hamiltonian
Dynamical Systems. <em>arXiv:2209.01521</em>. <a href=https://arxiv.org/pdf/2209.01521.pdf>Paper</a>.
<img loading=lazy src=/SISR.PNG alt="alt text"></p><ul><li><em>Fundamental Question</em>: How can we learn symbolic governing equations from observational data of physical dynamical systems?</li><li>We present a novel machine learning framework for answering this question. It works
in the following way:<ol><li>We use some specially constructed symplectic neural networks to deduce algebraic
properties of the governing equations, such as additive separability.</li><li>Next, an LSTM-RNN autoregressively generates a sequence of operators/system variables that
corresponds to the pre-order traversal of an expression tree for some symbolic equation. Specifically,
the LSTM-RNN is structured to generate separable physics equations called <a href=https://en.wikipedia.org/wiki/Hamiltonian_mechanics>Hamiltonians</a>.</li><li>We transpile the expression trees into Pytorch code so that we can optimize their constants with auto-differentiation.</li><li>We use the expression tree to time-evolve the physical system by symplectically integrating it. The error
that we get in doing so is our loss function.</li><li>Due to the transpiling step, we lose end-to-end differentiability. So, we train the LSTM-RNN with a reinforcement learning approach called risk-seeking policy gradients.</li><li>We go back to step 2 and repeat until we&rsquo;re satisfied with the generated equation.</li></ol></li><li><em>Code</em>: <a href=https://github.com/dandip/SISR>https://github.com/dandip/SISR</a></li><li><em>My Contribution</em>: This was my senior thesis&ndash;I&rsquo;m responsible for the implementation, nearly all of the ideation, and the paper. My advisor suggested step 1, which greatly improved performance. The variable-length batching code of the LSTM-RNN, as well as the transpiler, presented fun technical
challenges.</li></ul><hr><p><strong>DiPietro, D. M.</strong>, Xiong, S., & Zhu, B. (2020). Sparse Symplectically Integrated Neural Networks.
<em>Advances in Neural Information Processing Systems</em>. <a href=https://proceedings.neurips.cc/paper/2020/file/439fca360bc99c315c5882c4432ae7a4-Paper.pdf>Paper</a>.
<img loading=lazy src=/SSINN.PNG alt="alt text"></p><ul><li><em>Fundamental Question</em>: How can we learn symbolic governing equations from observational data of physical dynamical systems?</li><li>We present a novel machine learning framework for answering this question. It works
in the following way:<ol><li>Pick some finite function space, i.e. 6th order polynomials, perhaps with some trigonometric terms.</li><li>Implement a neural network that encodes this function space, with the coefficients of each term
being trainable parameters.</li><li>To obtain loss, symplectically integrate the neural network (making this a form of neural differential equation) and take the L1-error between its predicted
future state of the physical system and the physical systems actual future state. Train the network until
it converges (you have your equation) or fails to (pick a different function space).</li></ol></li><li><em>Code</em>: <a href=https://github.com/dandip/ssinn>https://github.com/dandip/ssinn</a></li><li><em>My Contribution</em>: My co-authors helped with the numerical integration code. I&rsquo;m otherwise responsible for the ideation, implemenation, and paper.</li></ul><hr><p>Deng, Y., Zhang, Y., He, X., Yang, S., Tong, Y., Zhang, M., <strong>DiPietro, D. M.</strong>, & Zhu, B. (2020).
Soft Multicopter Control using Neural Dynamics Identification. <em>Conference on Robot Learning</em>.
<a href=https://proceedings.mlr.press/v155/deng21a.html>Paper</a>.
<img loading=lazy src=/drone.PNG alt="alt text"></p><ul><li><em>Fundamental Question</em>: How do we effectively control a soft-bodied drone? These drones can deform
to get through small spaces and can crash without sustaining as much damage as a stiff drone.</li><li><em>My Contribution</em>: I wrote a soft-bodied physics engine used to generate training data for this paper.
My implementation used the finite element method and a neo-Hookean model of elasticity. It was written in C++ using the Eigen library and can run in real-time.</li></ul><h2 id=machine-learning-natural-language-processing>Machine Learning (Natural Language Processing)<a hidden class=anchor aria-hidden=true href=#machine-learning-natural-language-processing>#</a></h2><p><strong>DiPietro, D. M.</strong> (2022). Quantitative Stopword Generation for Sentiment Analysis via Recursive
and Iterative Deletion. <em>arXiv:2209.01519</em>. <a href=https://arxiv.org/pdf/2209.01519.pdf>Paper</a>.</p><ul><li><em>Fundamental Question</em>: How do we rigorously generate a set of stopwords?</li><li><em>My technique</em>:<ol><li>Take a pre-trained transformer (DistilBERT).</li><li>Perform an iterative feature deletion on every word in its vocabulary.</li><li>Rank the words in order of how much their deletion degrades performance (measured as AUC) on the test set.</li></ol></li><li>When training new models, my best stopword set reduced corpus size by 63.7% while only reducing
accuracy by 2.8%. A smaller stopword set reduced corpus size by 16% without affecting performance.</li></ul><hr><p><strong>DiPietro, D. M.</strong>, Hazari, V. D., & Vosoughi, S. (2022). Robin: A Novel Online Suicidal Text Corpus
of Substantial Breadth and Scale. <em>arXiv:2209.05707</em>. <a href=https://arxiv.org/pdf/2209.05707.pdf>Paper</a>.</p><ul><li><em>Fundamental Question</em>: How do we create models that accurate classify suicidal text?</li><li>We scrape 1.1m suicidal social media posts and use them to fine-tune a BERT model, achieving SOTA
on this task.</li><li><em>Code</em>: <a href=https://github.com/dandip/DH_Kappa>https://github.com/dandip/DH_Kappa</a></li><li><em>My Contribution</em>: I wrote all of the code&ndash;otherwise even contribution amongst co-authors.</li></ul><h2 id=statistics-theory>Statistics Theory<a hidden class=anchor aria-hidden=true href=#statistics-theory>#</a></h2><p><strong>DiPietro, D. M.</strong>, & Hazari, V. D. (2022). DiPietro-Hazari Kappa: A Novel Metric for Assessing
Labeling Quality via Annotation. <em>arXiv:2209.08243</em>. <a href=https://arxiv.org/pdf/2209.08243.pdf>Paper</a>.</p><ul><li><em>Fundamental Question</em>: Suppose a dataset is labeled via some heuristic. The same dataset is then
annotated by humans. How do we measure the &ldquo;goodness&rdquo; of our heuristic labels in the context of our
human annotation labels?</li><li><em>Our technique</em>: We present a novel statistical/probabilistic approach inspired by Fleiss&rsquo;s Kappa. At a high level, we measure the annotator agreement different that was attained above random chance. We also offer a performant matrix implementation.</li><li><em>My Contribution</em>: Ideation on this was 50/50 with my co-author. I formalized our math, derived the matrix formulation, and did the <a href=https://github.com/dandip/DH_Kappa>code implementation</a>.</li></ul><h2 id=quantitative-finance>Quantitative Finance<a hidden class=anchor aria-hidden=true href=#quantitative-finance>#</a></h2><p>Fleiss, A., Cui, H., Stoikov, S., & <strong>DiPietro, D. M.</strong> (2020). Constructing Equity Portfolios from SEC
13F Data Using Feature Extraction and Machine Learning. <em>Journal of Financial Data Science</em>, 2(1),
45-60. <a href=https://jfds.pm-research.com/content/early/2019/12/10/jfds.2019.1.022>Paper</a>. <em>not open access :(</em></p><ul><li><em>Fundamental Question</em>: How can we use SEC 13F data to quantitatively inform long-term investments?</li><li>We extract features and use them to train gradient-boosted trees. This strategy earned 19.8% annualized in historical backtesting versus 9.5% for the S&amp;P 500 over the same time period.</li></ul><p><strong>DiPietro, D. M.</strong> (2019). Alpha Cloning: Using Quantitative Techniques and SEC 13F Data for
Equity Portfolio Optimization and Generation. <em>Journal of Financial Data Science</em>, 1(4), 159-171.
<a href=https://jfds.pm-research.com/content/1/4/159.short>Paper</a>. <em>not open access :(</em></p><ul><li><em>Fundamental Question</em>: How can we use SEC 13F data to quantitatively inform long-term investments?</li><li>I employ some naive heuristics that are able to beat the S&amp;P 500 over a five-year backtested period (95% vs 72%).</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://dandipietro.com/>Dan DiPietro</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>